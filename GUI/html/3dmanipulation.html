<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="theme-color" content="#222222">
    <title>Interaction Lab - Interaction with Mobile</title>
    <link href="../css/bootstrap.css" rel="stylesheet">
    <link href="../css/modern-business.css" rel="stylesheet">
    <link href="../font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand home" href="../../index.html"> </a>
            </div>

            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li class="dropdown"><a href="#" class="dropdown-toggle" data-toggle="dropdown">EXTERNAL LINKS <b class="caret"></b></a><ul class="dropdown-menu"><li><a href="https://wiki.inf.ufrgs.br/Computer_Graphics,_Image_Processing_and_Interaction">Lab Wiki</a></li><li><a href="http://www.inf.ufrgs.br/~vajoliveira/LabCloud/index.html">LabCloud</a></li><li><a href="https://vimeo.com/groups/cgufrgs/videos">Vimeo</a></li></ul></li>
                </ul>
            </div>
        </div>
    </nav>
    <div class="section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="page-header">Interaction with Mobile<br/></h1>
                    <ol class="breadcrumb">
                        <li><a href="../../index.html">Home</a>
                        </li>
                        <li class="active">Interaction with Mobile</li>
                    </ol>
                </div>
            </div>

            <div class="row">
                
                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/CollaborativeAR.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Collaborative Manipulation of 3D Virtual Objects in Augmented Reality Scenarios Using Mobile Devices 
                    <br/><spam class="small">3DUI Contest 2017 (submitted)</spam></h3>

                    <p><b>Authors:</b> GRANDI, J. G., BERNDT, I., DEBARRBA, H. G., NEDEL, L. and MACIEL, A.</p>
                    <p><b>Abstract:</b> Interaction on augmented reality environments may be a very complex task, depending on the degrees of freedom required for the task. In this work we present a 3D user interface for collaborative manipulation of three-dimensional objects in augmented reality (AR) environments.Itmapsposition–acquiredwithacameraandﬁducial markers – and touchscreen input of a handheld device into gestures to select, move, rotate and scale virtual objects. As these transformations require the control of multiple degrees of freedom (DOFs), collaboration is proposed as a solution to coordinate the modiﬁcation of each and all the available DOFs. Users are free to decide their own manipulation roles. All virtual elements are displayed directly in the mobile device as an overlay of the camera capture, providing an individual point of view of the AR environment to each user. </p>

                    [<a href="../../PAPERS/3dmanipulation/CollaborativeAR.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
					<img class="img-responsive" src="../images/acadcover2.png" alt="Collaborative 3DUI" />
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Design and Evaluation of a Handheld-based 3D User Interface for Collaborative Object Manipulation
                    <br/><spam class="small">CHI 2017 (to appear)</spam></h3>

                    <p><b>Authors:</b> GRANDI, J. G., DEBARRBA, H. G., NEDEL, L. and MACIEL, A. </p>
                    <p><b>Abstract:</b> Object manipulation in 3D virtual environments demands a combined coordination of rotations, translations and scales, as well as the camera control to change the user’s viewpoint. Then, for many manipulation tasks, it would be advantageous to share the interaction complexity among team members. In this paper we propose a novel 3D manipulation interface based on a collaborative action coordination approach. Our technique explores a smartphone – the touchscreen and inertial sensors – as input interface, enabling several users to collaboratively manipulate the same virtual object with their own devices. We first assessed our interface design on a docking and an obstacle crossing tasks with teams of two users. Then, we conducted a study with 60 users to understand the influence of group size in collaborative 3D manipulation. We evaluated teams in combinations of one, two, three and four participants. Experimental results show that teamwork increases accuracy when compared with a single user. The accuracy increase is correlated with the number of individuals in the team and their work division strategy.</p>

                    [<a href="../../PAPERS/3dmanipulation/Collaborative3DUI_CHI.pdf">paper</a>] 
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/Collaborative3DUI.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Collaborative 3D Manipulation using Mobile Phones
                    <br/><spam class="small">3DUI Contest 2016</spam></h3>

                    <p><b>Authors:</b> GRANDI, J. G., BERNDT, I., DEBARRBA, H. G., NEDEL, L. and MACIEL, A. </p>
                    <p><b>Abstract:</b> We present a 3D user interface for collaborative manipulation of three-dimensional objects in virtual environments. It maps inertial sensors, touch screen and physical buttons of a mobile phone into well-known gestures to alter the position, rotation and scale of virtual objects. As these transformations require the control of multiple degrees of freedom (DOF), collaboration is proposed as a solution to coordinate the modification of each and all the available DOFs. Users are free to define their manipulation roles. All virtual elements are displayed in a single shared screen, which is handy to aggregate multiple users in the same physical space.</p>

					<p><img class="img-responsive" src="../images/prize_winner.png" alt="Best 3DUI" title="Best 3DUI Contest Award" style="display:inline"> <spam class="prizetxt">Best 3DUI Contest Award</spam></p>

                    [<a href="../../PAPERS/3dmanipulation/Collaborative3DUI.pdf">paper</a>] 
                </div>
                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/SecondScreenSVR2015.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Guidelines for Designing Dynamic Applications With Second Screen
                    <br/><spam class="small">SVR 2015</spam></h3>

                    <p><b>Authors:</b> Bruno Pagno, Diogo Costa, Leandro Guedes, Carla Dal Sasso Freitas and Luciana Nedel</p>
                    <p><b>Abstract:</b> The concept of second screen became popular with the introduction of interactive TVs. In this context, while the user focuses on the TV screen, the exploration of additional content is possible through the use of a smartphone or tablet as a second screen. Lately, dynamic applications, e.g. video games, also started to use a second screen. Nintendo DS and Wii U are the game consoles that began to incorporate these ideas. Dynamic applications are based on real time action and interaction, and their implementation can be very complex specially because users have to change focus between the displays frequently. In this paper, we summarize the results found in a set of experimental studies we conducted to verify the usability of interaction techniques based on the use of a second auxiliary screen in dynamic applications. We developed a multiplayer game that employs one main screen shared by two players, each one also using a second (private) screen. From these studies, we elaborate a set of guidelines to help developers in the use of second screens. Although future case studies would improve these guidelines, our experiments show that they contribute with robust principles for developers who want to build multiscreen applications.</p>

                    [<a href="../../PAPERS/3dmanipulation/SecondScreenSVR2015.pdf">paper</a>] 
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>


                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/LivAR.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Spatially Aware Mobile Interface for 3D Visualization and Interactive Surgery Planning
                    <br/><spam class="small">SEGAH 2014</spam></h3>

                    <p><b>Authors:</b> GRANDI, J. G., DEBARBA, H. G., ZANCHET, D. J. and MACIEL, A.</p>
                    <p><b>Abstract:</b> While medical images are fundamental in the surgery planning procedure, the process of analysis of such images slice-by-slice is still tedious and inefficient. In this work we introduce a system for exploration of the internal anatomy structures directly on the surface of the real body using a mobile display device as a window to the interior of the patient’s body. The method is based on volume visualization of standard computed tomography datasets and augmented reality for interactive visualization of the generated volume. It supports our liver surgery planner method in the analysis of the segmented liver and in the color classification of the vessels. We present a set of experiments showing the system’s ability to operate on mobile devices. Quantitative performance results are detailed, and applications in teaching anatomy and doctor-patient communication are discussed.</p>

                    [<a href="../../PAPERS/3dmanipulation/LivAR.pdf">paper</a>] 
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/ThePointWalker.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">The Point Walker Multi-label Approach
                    <br/><spam class="small">3DUI Contest 2014</spam></h3>

                    <p><b>Authors:</b> Hernandi Krammes, Marcio M. Silva, Theodoro Mota, Matheus T. Tura, Anderson Maciel, Luciana Nedel</p>
                    <p><b>Abstract:</b> This paper presents a 3D user interface to select and label point sets in a point cloud. A walk-in-place strategy based on a weight platform is used for navigation. Selection is made in two levels of precision. First, a pointing technique is used relying on a smartphone and built-in sensors. Then, an ellipsoidal selection volume is deformed by pinching on the smartphone touchscreen in different orientations. Labels are finally selected by pointing icons and a hierarchy of labels is automatically defined by multiple labelling. Voice is used to create new icons/labels. The paper describes the concepts in our approach and the system implementation.</p>

                    [<a href="../../PAPERS/3dmanipulation/3DUI14.pdf">paper</a>] 
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>


                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/DisambiguationCanvas.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Disambiguation Canvas: A Precise Selection Technique for Virtual Environments
                    <br/><spam class="small">INTERACT 2013</spam></h3>

                    <p><b>Authors:</b> DEBARBA, H. G., GRANDI, J. G., MACIEL, A., NEDEL, L. and BOULIC R.</p>
                    <p><b>Abstract:</b> We present the disambiguation canvas, a technique developed for easy, accurate and fast selection of small objects and objects inside cluttered virtual environments. Disambiguation canvas rely on selection by progressive refinement, it uses a mobile device and consists of two steps. During the first, the user defines a subset of objects by means of the orientation sensors of the device and a volume casting pointing technique. The subsequent step consists of the disambiguation of the desired target among the previously defined subset of objects, and is accomplished using the mobile device touchscreen. By relying on the touchscreen for the last step, the user can disambiguate among hundreds of objects at once. User tests show that our technique performs faster than ray-casting for targets with approximately 0.53 degrees of angular size, and is also much more accurate for all the tested target sizes.</p>

                    [<a href="../../PAPERS/3dmanipulation/DisambiguationCanvas.pdf">paper</a>] 
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div> 

                <div class="col-md-4">
                    <img class="img-responsive" src="../images/svr2013.png" alt="svr2013" />
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Study of the Sensors Embedded in Smartphones for Use in Indoor Localization
                    <br/><spam class="small">SVR 2013</spam></h3>

                    <p><b>Authors:</b>Ivan Boesing, Tomaz Silva and Luciana Nedel</p>
                    <p><b>Abstract:</b> In recent years, smartphones experienced a fast technological growth, approaching its computational capacity to personal computers. In addition to that, new hardware and sensors are being incorporated into these devices. The smartphone’s embedded sensors are a low cost solution that allows interactions between humans, computers and the environment. Examples include applications designed to identify the user’s location by GPS receiver, games that use accelerometers and/or gyroscope, Wi-Fi and Bluetooth antennas that exchange information between users, microphones that perceive user’s gestural movements, and so on. This paper presents a study of the various smartphones sensors, detailing their operation and identifying their main features, advantages, drawbacks, and potential of use for indoor location. We analyzed smartphones sensitive to sound, Wi-Fi, magnetic field, 3-dimensional linear acceleration and angular velocity. This study presented – as the ideal solution – a fusion of sensors to complement individual skills and to improve the information accuracy regarding the device’s rotation and translation. </p>

                    [<a href="../../PAPERS/3dmanipulation/svr2013.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/LOP.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">LOP-cursor: Fast and precise interaction with tiled displays using one hand and levels of precision
                    <br/><spam class="small">3DUI 2012</spam></h3>

                    <p><b>Authors:</b> DEBARBA, H. G., NEDEL, L. and MACIEL, A.</p>
                    <p><b>Abstract:</b> We present levels of precision (LOP) cursor, a metaphor for high precision pointing and simultaneous cursor controlling using commodity mobile devices. The LOP-cursor uses a two levels of precision representation that can be combined to access low and high resolution of input. It provides a constrained area of high resolution input and a broader area of lower input resolution, offering the possibility of working with a two legs cursor using only one hand. LOP-cursor is designed for interaction with large high resolution displays, e.g. display walls, and distributed screens/computers scenarios. This paper presents the design of the cursor, the implementation of a prototype, and user evaluation experiments showing that our method allows both, the acquisition of small targets, and fast interaction while using simultaneous cursors in a comfortable manner. Targets smaller than 0.3 cm can be selected by users at distances over 1.5 m from the screen with minimum effort.</p>

                   	[<a href="../../PAPERS/3dmanipulation/LOPCursor.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/CubeOfDoom.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">The cube of doom: A bimanual perceptual user Experience
                    <br/><spam class="small">3DUI Contest 2011</spam></h3>

                    <p><b>Authors:</b> DEBARBA, H. G., FRANZ, J., REUS, V., MACIEL, A. and NEDEL, L.</p>
                    <p><b>Abstract:</b> This paper presents a 3D user interface to solve a three-dimensional wooden blocks puzzle. Such interface aims at reproducing the real scenario of puzzle solving using involving devices and techniques for interaction and visualization which include a mobile device, haptics and enhanced stereo vision. The paper describes our interaction approach, the system implementation and user experiments</p>

                   	[<a href="../../PAPERS/3dmanipulation/CubeOfDoom.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/Peep.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Reality Cues-Based Interaction Using Whole-Body Awareness 
                    <br/><spam class="small">SAC 2010</spam></h3>

                    <p><b>Authors:</b> MACIEL, A. and NEDEL, L., JORGE, V. A. M., IBIAPINA, J. M. T., SILVA, L. F. M. S.</p>
                    <p><b>Abstract:</b> The exploration of 3D environments using 6 degrees-of-freedom interaction is still a challenge since users easily become disoriented. In this paper we discuss the benefits of the whole-body awareness in 3D interactive applications. We propose a technique for navigation and selection in 3D environments which explores the peephole metaphor with a tablet PC.In practice, the tablet is held by the participant who moves it around and points it in any direction for visualization and interaction. The method was tested with a set of users who were asked to perform selection tasks. The technique presented competitive results when compared with conventional interaction methods and also showed that real world body orientation memory helps users to perform better in the virtual world.</p>

                   	[<a href="../../PAPERS/3dmanipulation/Peep.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

              <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/TabletSAC10.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Collaborative Interaction through Spatially Aware Moving Displays 
                    <br/><spam class="small">SAC 2010</spam></h3>

                    <p><b>Authors:</b>Anderson Maciel, Luciana P. Nedel, Eduardo M. Mesquita, Marcelo H. Mattos, Gustavo M. Machado, Carla M.D.S. Freitas </p>
                    <p><b>Abstract:</b> In many real life situations, people work together using each their own computers. In practice, besides personal communication, such situations often involve exchanging documents and other digital objects. Since people are working in a common physical space, it is a natural idea to enlarge the virtual space to a common area where they can exchange objects while taking advantage of the collaborators' physical proximity. In this work we propose a way to allow collaboration through the interaction with objects in a common virtual workspace built on the top of tablet PCs. The concepts of dynamic multiple displays and real world position tracking are implemented exploiting the tablet's embodied resources such as webcam, touch-screen and stylus. Also, a multiplayer game was implemented to show how users can exchange information through intercommunicating tablets. We performed user tests to demonstrate the feasibility of collaborative tasks in such environment, and drawn conslusions regarding the impact of the new paradigm of extended multi-user workspaces.</p>

                   	[<a href="../../PAPERS/3dmanipulation/TabletSAC10.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/ARsightseeingSVR10.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">Permeating the Architectural Past in Dante Alighieri Square in Caxias do Sul through Augmented Reality 
                    <br/><spam class="small">SIBGRAPI 2010</spam></h3>

                    <p><b>Authors:</b> RIBOLDI, G., MACIEL, A.</p>
                    <p><b>Abstract:</b> Important buildings and urban sites of the recent past were not adequately documented and have been forgotten for a long time. In this context, new experiences with the 3D modeling and simulation of such spaces in VR are becoming very impactful as documentation tools. However, these virtual spaces are not accessible to the general public as visualization tools are not available. The purpose of this work, then, is to create an interaction environment in augmented reality to explore historical areas in such a way that ancient versions of buildings can be visualized and explored directly in the real space where the buildings were in the past or their current versions are situated today. Users handling a mobile display device, as a tablet PC, walk around the real site, and as they point the display towards a neighboring building, they can see how it was in the past, which allows a travel in time, offering a fourth dimension to the experience. The results obtained demonstrate the potential of augmented reality applications for the dissemination of historical heritage.</p>

                   	[<a href="../../PAPERS/3dmanipulation/ARchitectural_pastSIBGRAPI10.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div> 

                <div class="col-md-4">
                    <img class="img-responsive" src="../images/sibwuw2010b.jpg" alt="sibwuw2010b" />
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">An Interactive Dynamic Tiled Display System 
                    <br/><spam class="small">SIBGRAPI 2010</spam></h3>

                    <p><b>Authors:</b>Juliano Franz, Gelson Reinaldo, Anderson Maciel and Luciana Nedel</p>
                    <p><b>Abstract:</b> Data acquisition devices and algorithms are generating each day larger datasets. As displays are not evolving in the same velocity, the use of tiled displays systems is being seriously considered for the visualization of huge datasets. However, tiled-displays are expensive and large, requiring dedicated rooms for it. Therefore we propose a low cost and scalable tiled display using an array of movable tablet PCs. We also present a strategy to interact with applications running on this dynamic tiled display system, which can be operated by one or multiple users concurrently. Our solution is based on two principles: even if each tile is a separate computer, users should feel it as an unique application running on a single machine; interaction is provided by sketching gestures directly over the displays surfaces using the tablet stylus. Users may use the system in a natural way, as they are just taking notes in their own scrapbook. Preliminary results are presented and discussed.</p>

                   	[<a href="../../PAPERS/3dmanipulation/sibwuw2010b.pdf">paper</a>]
                </div>

                <div class="col-md-12">
                    <h2 class="page-header"> </h2>
                </div>   

                <div class="col-md-4">
                    <video width="100%" height="100%" controls>
                        <source src="../../VIDEOS/3dmanipulation/svr-tablet-ing.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8">
                    <h3 style="margin-top:0;">A Dynamic Multi-display System Approach 
                    <br/><spam class="small">SVR 2008</spam></h3>

                    <p><b>Authors:</b>Gelson C. Reinaldo, Marilena Maule, Márcio Zacarias, Anderson Maciel, Carla M.D.S. Freitas, Luciana P. Nedel</p>
                    <p><b>Abstract:</b> Techniques for construction and configuration of tiled display systems have been focused by a number of research groups. Arrays of monitors or projectors in a fixed size matrix NxM can be managed by computer clusters to display a single image with large dimensions and high resolution. In the present work, an array of tablet PCs is used to compose a tiled display with a specific interaction feature due to the mobility of each individual tablet. Tracking ground fixed markers with the tablets web cameras enable the system to change the virtual image region to be displayed by each tile, which allows dynamic exploration of the visualization space.</p>

                    [<a href="../../PAPERS/3dmanipulation/svr-tablet-ing.pdf">paper</a>]
                </div>            

            </div>
        </div>
    </div>
    <div class="container">
        <hr>
        <footer>
            <div class="row">
                <div class="col-lg-12">
                <a href="http://www.ufrgs.br/ufrgs/inicial" class="logo"><img src="../images/ufrgs.png" alt="logo da UFRGS" title="Universidade Federal do Rio Grande do Sul" height="50px"/></a>
                <a href="http://www.inf.ufrgs.br/" class="logo"><img src="../images/inf.png" alt="logo do INF" title="Instituto de Informática UFRGS"  height="50px"/></a>
                <a href="https://wiki.inf.ufrgs.br/Computer_Graphics,_Image_Processing_and_Interaction" class="logo"><img src="../images/logo.png" alt="logo do Laboratório de Computação Gráfica, Visualização e Interação" title="Laboratório de Computação Gráfica, Visualização e Interação"  height="50px"/></a>
                <p>Instituto de Informática - UFRGS<br/>
                    Caixa Postal 15064, CEP: 91501-970 Porto Alegre - RS - Brasil<br/>
                    Fone +55 (51) 3308-6168 / Fax +55 (51) 3308-7308</p>
                </div>
            </div>
        </footer>
    </div>

    <script src="../js/jquery-1.10.2.js"></script>
    <script src="../js/bootstrap.js"></script>
    <script src="../js/modern-business.js"></script>
    <noscript>
        <p>O JavaScript não está funcionando, você pode ter problemas ao utilizar esta página.</p>
    </noscript>
</body>
</html>
